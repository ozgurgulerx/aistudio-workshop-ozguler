{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ozgurguler/Developer/Projects/aistudio-workshop-ozguler/pf/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from promptflow.tracing import trace\n",
    "from promptflow.core import Prompty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ozgurguler/Developer/Projects/aistudio-workshop-ozguler/code\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_DIR = Path(os.getcwd()).absolute()\n",
    "print(BASE_DIR)\n",
    "# determines the path to the directory containing the current script file and stores it in the variable BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trace\n",
    "def chat(question: str = \"What's the capital of France?\") -> str:\n",
    "    \"\"\"Flow entry function.\"\"\"\n",
    "\n",
    "    if \"OPENAI_API_KEY\" not in os.environ and \"AZURE_OPENAI_API_KEY\" not in os.environ:\n",
    "        # load environment variables from .env file\n",
    "        load_dotenv()\n",
    "\n",
    "    prompty = Prompty.load(source=BASE_DIR / \"chat.prompty\")\n",
    "    # trigger a llm call with the prompty obj\n",
    "    output = prompty(question=question)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat function, decorated with @trace for logging or monitoring, checks if specific API keys are set in the environment. If not, it loads them from a .env file. It then loads a Prompty object from a chat.prompty file and uses this object to call a language model with the provided question, returning the model’s response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "Quantum entanglement is a phenomenon in quantum physics where two or more particles become connected in such a way that the state of one particle is dependent on the state of the other, regardless of the distance between them. This means that measuring the state of one particle instantaneously determines the state of the other, even if they are separated by vast distances. This concept challenges our classical understanding of cause and effect, as the entangled particles seem to be able to communicate with each other faster than the speed of light. Quantum entanglement is a fundamental aspect of quantum mechanics and has been experimentally verified through various experiments. It also plays a crucial role in fields such as quantum computing and quantum cryptography.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from promptflow.tracing import start_trace\n",
    "\n",
    "    start_trace()\n",
    "\n",
    "    result = chat(\"What is quantum entanglement?\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block checks if the script is being run directly (not imported as a module) using if __name__ == \"__main__\":. It then imports start_trace from promptflow.tracing and calls it to begin tracing. Finally, it calls the chat function with the question “What is quantum entanglement?” and prints the result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
